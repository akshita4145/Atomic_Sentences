{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 252\n",
      "  sentence_id  \\\n",
      "0           1   \n",
      "1           1   \n",
      "2           1   \n",
      "3           2   \n",
      "4           2   \n",
      "\n",
      "                                                                                             model_atomic  \\\n",
      "0                  Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet.   \n",
      "1                                                               Jakobshavn Isbr passing out of the fjord.   \n",
      "2  Some 10  of all Greenland icebergs some 35 billion tonnes of icebergs calved passing out of the fjord.   \n",
      "3                                                                       Wright played the part of mufasa.   \n",
      "4                                          Wright voiced the iguanodon in disney 's cgi film '' dinosaur.   \n",
      "\n",
      "                                                                                            gold_atomic  \n",
      "0               Jakobshavn Isbræ is a major contributor to the mass balance of the Greenland ice sheet.  \n",
      "1                        Jakobshavn Isbræ releases large icebergs that pass out of the fjord each year.  \n",
      "2  Around 10% of Greenland’s icebergs—about 35 billion tonnes—calve and flow out of the fjord annually.  \n",
      "3              Wright played the role of Mufasa in the original Broadway production of *The Lion King*.  \n",
      "4                                    He also voiced Kron the Iguanodon in Disney’s CGI film *Dinosaur*.  \n",
      "the cat in the hat ['the', 'cat', 'in', 'the', 'hat']\n",
      "\n",
      "=== ROW-LEVEL METRICS ===\n",
      "Exact match rate          : 0.03571428571428571\n",
      "Token-level F1 (mean)     : 0.6146256448260873\n",
      "Token-level precision mean: 0.6933529460676624\n",
      "Token-level recall mean   : 0.5855549378359961\n",
      "\n",
      "=== SET-LEVEL METRICS (per sentence_id) ===\n",
      "Macro precision: 0.03743961352657004\n",
      "Macro recall   : 0.03743961352657004\n",
      "Macro F1       : 0.03743961352657004\n",
      "\n",
      "Micro precision: 0.03571428571428571\n",
      "Micro recall   : 0.03571428571428571\n",
      "Micro F1       : 0.03571428571428571\n",
      "\n",
      "=== WORST ROW-LEVEL F1 EXAMPLES ===\n",
      "     sentence_id  \\\n",
      "115          101   \n",
      "190  sentence_id   \n",
      "212          158   \n",
      "16            10   \n",
      "72            77   \n",
      "13             9   \n",
      "193          147   \n",
      "94            89   \n",
      "246          177   \n",
      "11             6   \n",
      "\n",
      "                                                                              model_atomic  \\\n",
      "115                                                                            It went up.   \n",
      "190                                                                           model_atomic   \n",
      "212                                                             He could not be explained.   \n",
      "16                          Any two unmarried people come with marriages and civil unions.   \n",
      "72                                                                Band first single feels.   \n",
      "13                                          The vicious white kids formed for one concert.   \n",
      "193                                                                     X got ta tell you.   \n",
      "94                                                              In practise she took over.   \n",
      "246  A new theoretical framework condensed during mass loss from stars of differing types.   \n",
      "11                                        The lead track combines the band 's first album.   \n",
      "\n",
      "                                                                     gold_atomic  \\\n",
      "115                         The total amount eventually increased significantly.   \n",
      "190                                                                  gold_atomic   \n",
      "212                                 Certain textual issues remained unexplained.   \n",
      "16       This arrangement grants some marriage-like rights to unmarried couples.   \n",
      "72       The band’s debut single conveys the emotional tone of their early work.   \n",
      "13                                They came together solely to perform one show.   \n",
      "193                      “Gotta Tell You” became a major hit for Samantha Mumba.   \n",
      "94                               She ultimately assumed control of the business.   \n",
      "246  This framework described mass-loss behavior across different stellar types.   \n",
      "11       The track fuses elements of their early and more recent musical styles.   \n",
      "\n",
      "     row_token_f1  \n",
      "115      0.000000  \n",
      "190      0.000000  \n",
      "212      0.000000  \n",
      "16       0.105263  \n",
      "72       0.125000  \n",
      "13       0.125000  \n",
      "193      0.133333  \n",
      "94       0.166667  \n",
      "246      0.181818  \n",
      "11       0.190476  \n",
      "\n",
      "=== WORST SET-LEVEL F1 SENTENCE_IDS ===\n",
      "  sentence_id  n_model  n_gold  set_precision  set_recall  set_f1  \\\n",
      "0           1        3       3            0.0         0.0     0.0   \n",
      "1          10        3       3            0.0         0.0     0.0   \n",
      "2         100        2       2            0.0         0.0     0.0   \n",
      "3         101        1       1            0.0         0.0     0.0   \n",
      "4         102        2       2            0.0         0.0     0.0   \n",
      "5         103        2       2            0.0         0.0     0.0   \n",
      "6         104        2       2            0.0         0.0     0.0   \n",
      "7         105        1       1            0.0         0.0     0.0   \n",
      "8         106        2       2            0.0         0.0     0.0   \n",
      "9         107        3       3            0.0         0.0     0.0   \n",
      "\n",
      "   true_positives  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "5               0  \n",
      "6               0  \n",
      "7               0  \n",
      "8               0  \n",
      "9               0  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "DATA_PATH = \"../data/gold_standard.csv\"   # e.g., \"atomic_eval.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "required_cols = {\"sentence_id\", \"model_atomic\", \"gold_atomic\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return normalize_text(s).split()\n",
    "\n",
    "print(normalize_text(\"The Cat, in THE Hat!!\"), tokenize(\"The Cat, in THE Hat!!\"))\n",
    "\n",
    "def token_f1(pred: str, gold: str):\n",
    "    pred_toks = tokenize(pred)\n",
    "    gold_toks = tokenize(gold)\n",
    "\n",
    "    if len(pred_toks) == 0 and len(gold_toks) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    if len(pred_toks) == 0 or len(gold_toks) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    pred_counts = Counter(pred_toks)\n",
    "    gold_counts = Counter(gold_toks)\n",
    "    overlap = sum((pred_counts & gold_counts).values())\n",
    "\n",
    "    precision = overlap / max(len(pred_toks), 1)\n",
    "    recall = overlap / max(len(gold_toks), 1)\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "df[\"norm_model\"] = df[\"model_atomic\"].apply(normalize_text)\n",
    "df[\"norm_gold\"] = df[\"gold_atomic\"].apply(normalize_text)\n",
    "\n",
    "df[\"exact_match\"] = (df[\"norm_model\"] == df[\"norm_gold\"]).astype(int)\n",
    "\n",
    "prs, rcs, f1s = [], [], []\n",
    "for m, g in zip(df[\"model_atomic\"], df[\"gold_atomic\"]):\n",
    "    p, r, f = token_f1(m, g)\n",
    "    prs.append(p)\n",
    "    rcs.append(r)\n",
    "    f1s.append(f)\n",
    "\n",
    "df[\"row_token_precision\"] = prs\n",
    "df[\"row_token_recall\"] = rcs\n",
    "df[\"row_token_f1\"] = f1s\n",
    "\n",
    "grouped = defaultdict(lambda: {\"gold\": set(), \"model\": set()})\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sid = row[\"sentence_id\"]\n",
    "    g = normalize_text(row[\"gold_atomic\"])\n",
    "    m = normalize_text(row[\"model_atomic\"])\n",
    "    if g:\n",
    "        grouped[sid][\"gold\"].add(g)\n",
    "    if m:\n",
    "        grouped[sid][\"model\"].add(m)\n",
    "\n",
    "def prf_from_sets(model_set, gold_set):\n",
    "    \"\"\"\n",
    "    Precision/Recall/F1 from two sets of strings (model vs gold).\n",
    "    \"\"\"\n",
    "    if len(model_set) == 0 and len(gold_set) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    if len(model_set) == 0 or len(gold_set) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    inter = model_set & gold_set\n",
    "    tp = len(inter)\n",
    "    precision = tp / len(model_set) if model_set else 0.0\n",
    "    recall = tp / len(gold_set) if gold_set else 0.0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "rows = []\n",
    "micro_tp = micro_pred = micro_gold = 0\n",
    "\n",
    "for sid, d in grouped.items():\n",
    "    mset, gset = d[\"model\"], d[\"gold\"]\n",
    "    p, r, f = prf_from_sets(mset, gset)\n",
    "\n",
    "    inter = mset & gset\n",
    "    micro_tp += len(inter)\n",
    "    micro_pred += len(mset)\n",
    "    micro_gold += len(gset)\n",
    "\n",
    "    rows.append({\n",
    "        \"sentence_id\": sid,\n",
    "        \"n_model\": len(mset),\n",
    "        \"n_gold\": len(gset),\n",
    "        \"set_precision\": p,\n",
    "        \"set_recall\": r,\n",
    "        \"set_f1\": f,\n",
    "        \"true_positives\": len(inter),\n",
    "    })\n",
    "\n",
    "set_df = pd.DataFrame(rows).sort_values(\"sentence_id\").reset_index(drop=True)\n",
    "\n",
    "macro_p = set_df[\"set_precision\"].mean()\n",
    "macro_r = set_df[\"set_recall\"].mean()\n",
    "macro_f = set_df[\"set_f1\"].mean()\n",
    "\n",
    "micro_p = micro_tp / micro_pred if micro_pred else 0.0\n",
    "micro_r = micro_tp / micro_gold if micro_gold else 0.0\n",
    "micro_f = 2 * micro_p * micro_r / (micro_p + micro_r) if (micro_p + micro_r) else 0.0\n",
    "\n",
    "print(\"\\n=== SET-LEVEL METRICS (per sentence_id) ===\")\n",
    "print(\"Macro precision:\", macro_p)\n",
    "print(\"Macro recall   :\", macro_r)\n",
    "print(\"Macro F1       :\", macro_f)\n",
    "print()\n",
    "print(\"Micro precision:\", micro_p)\n",
    "print(\"Micro recall   :\", micro_r)\n",
    "print(\"Micro F1       :\", micro_f)\n",
    "\n",
    "set_df.head()\n",
    "\n",
    "\n",
    "# 5. Inspect worst cases ----------------------------------------------------\n",
    "\n",
    "print(\"\\n=== WORST ROW-LEVEL F1 EXAMPLES ===\")\n",
    "worst_rows = df.sort_values(\"row_token_f1\").head(10)\n",
    "print(worst_rows[[\"sentence_id\", \"model_atomic\", \"gold_atomic\", \"row_token_f1\"]])\n",
    "\n",
    "print(\"\\n=== WORST SET-LEVEL F1 SENTENCE_IDS ===\")\n",
    "print(set_df.sort_values(\"set_f1\").head(10))\n",
    "\n",
    "\n",
    "# 6. Helper to inspect one sentence_id in detail ----------------------------\n",
    "\n",
    "def inspect_sentence_id(sid):\n",
    "    \"\"\"\n",
    "    Show all model/gold pairs for a given sentence_id + set-level TP/FP/FN.\n",
    "    \"\"\"\n",
    "    subset = df[df[\"sentence_id\"] == sid].copy()\n",
    "    if subset.empty:\n",
    "        print(f\"No rows with sentence_id={sid}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n================ sentence_id = {sid} ================\\n\")\n",
    "    print(\"Row-wise pairs:\")\n",
    "    display(subset[[\"model_atomic\", \"gold_atomic\", \"row_token_f1\"]])\n",
    "\n",
    "    mset = {normalize_text(x) for x in subset[\"model_atomic\"] if isinstance(x, str)}\n",
    "    gset = {normalize_text(x) for x in subset[\"gold_atomic\"] if isinstance(x, str)}\n",
    "    inter = mset & gset\n",
    "\n",
    "    print(\"\\nGold set (normalized):\")\n",
    "    for g in gset:\n",
    "        print(\"  G:\", g)\n",
    "\n",
    "    print(\"\\nModel set (normalized):\")\n",
    "    for m in mset:\n",
    "        print(\"  M:\", m)\n",
    "\n",
    "    print(\"\\nTrue positives (intersection):\")\n",
    "    for t in inter:\n",
    "        print(\"  TP:\", t)\n",
    "\n",
    "    print(\"\\nFalse negatives (gold but not model):\")\n",
    "    for fn in sorted(gset - inter):\n",
    "        print(\"  FN:\", fn)\n",
    "\n",
    "    print(\"\\nFalse positives (model but not gold):\")\n",
    "    for fp in sorted(mset - inter):\n",
    "        print(\"  FP:\", fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'rouge_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mrouge_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m rouge_scorer\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbert_score\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m score \u001b[38;5;28;01mas\u001b[39;00m bert_score\n\u001b[32m      7\u001b[39m pd.set_option(\u001b[33m\"\u001b[39m\u001b[33mdisplay.max_colwidth\u001b[39m\u001b[33m\"\u001b[39m, \u001b[32m200\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'rouge_score'"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "csv_path = \"atomic_eval_pairs.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "expected_cols = {\"model_atomic\", \"gold_atomic\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV is missing columns: {missing}. Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Loaded rows: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "def normalize_text(s):\n",
    "    if isinstance(s, str):\n",
    "        return \" \".join(s.split())\n",
    "    return \"\"\n",
    "\n",
    "df[\"model_atomic_norm\"] = df[\"model_atomic\"].apply(normalize_text)\n",
    "df[\"gold_atomic_norm\"]  = df[\"gold_atomic\"].apply(normalize_text)\n",
    "\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_p, rouge1_r, rouge1_f = [], [], []\n",
    "rouge2_p, rouge2_r, rouge2_f = [], [], []\n",
    "rougel_p, rougel_r, rougel_f = [], [], []\n",
    "\n",
    "for m, g in zip(df[\"model_atomic_norm\"], df[\"gold_atomic_norm\"]):\n",
    "    scores = scorer.score(g, m) \n",
    "\n",
    "    r1 = scores[\"rouge1\"]\n",
    "    r2 = scores[\"rouge2\"]\n",
    "    rl = scores[\"rougeL\"]\n",
    "\n",
    "    rouge1_p.append(r1.precision)\n",
    "    rouge1_r.append(r1.recall)\n",
    "    rouge1_f.append(r1.fmeasure)\n",
    "\n",
    "    rouge2_p.append(r2.precision)\n",
    "    rouge2_r.append(r2.recall)\n",
    "    rouge2_f.append(r2.fmeasure)\n",
    "\n",
    "    rougel_p.append(rl.precision)\n",
    "    rougel_r.append(rl.recall)\n",
    "    rougel_f.append(rl.fmeasure)\n",
    "\n",
    "# Add per-row ROUGE-L F1 to the dataframe (often the most interpretable)\n",
    "df[\"rougeL_f\"] = rougel_f\n",
    "\n",
    "print(\"\\n=== ROUGE (averaged over all pairs) ===\")\n",
    "print(f\"ROUGE-1 Precision: {np.mean(rouge1_p):.4f}\")\n",
    "print(f\"ROUGE-1 Recall: {np.mean(rouge1_r):.4f}\")\n",
    "print(f\"ROUGE-1 F1: {np.mean(rouge1_f):.4f}\\n\")\n",
    "\n",
    "print(f\"ROUGE-2 Precision: {np.mean(rouge2_p):.4f}\")\n",
    "print(f\"ROUGE-2 Recall: {np.mean(rouge2_r):.4f}\")\n",
    "print(f\"ROUGE-2 F1: {np.mean(rouge2_f):.4f}\\n\")\n",
    "\n",
    "print(f\"ROUGE-L Precision: {np.mean(rougel_p):.4f}\")\n",
    "print(f\"ROUGE-L Recall: {np.mean(rougel_r):.4f}\")\n",
    "print(f\"ROUGE-L F1: {np.mean(rougel_f):.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "cands = df[\"model_atomic_norm\"].tolist()\n",
    "refs  = df[\"gold_atomic_norm\"].tolist()\n",
    "\n",
    "P, R, F1 = bert_score(cands, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "# Convert tensors to floats\n",
    "df[\"bertscore_P\"]  = P.numpy()\n",
    "df[\"bertscore_R\"]  = R.numpy()\n",
    "df[\"bertscore_F1\"] = F1.numpy()\n",
    "\n",
    "print(\"\\n=== BERTScore (averaged over all pairs) ===\")\n",
    "print(f\"BERTScore Precision: {df['bertscore_P'].mean():.4f}\")\n",
    "print(f\"BERTScore Recall   : {df['bertscore_R'].mean():.4f}\")\n",
    "print(f\"BERTScore F1       : {df['bertscore_F1'].mean():.4f}\")\n",
    "\n",
    "\n",
    "\n",
    "out_path = \"atomic_eval_with_rouge_bertscore.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved detailed results to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
