{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded rows: 252\n",
      "  sentence_id  \\\n",
      "0           1   \n",
      "1           1   \n",
      "2           1   \n",
      "3           2   \n",
      "4           2   \n",
      "\n",
      "                                                                                             model_atomic  \\\n",
      "0                  Jakobshavn Isbr is a major contributor to the mass balance of the Greenland ice sheet.   \n",
      "1                                                               Jakobshavn Isbr passing out of the fjord.   \n",
      "2  Some 10  of all Greenland icebergs some 35 billion tonnes of icebergs calved passing out of the fjord.   \n",
      "3                                                                       Wright played the part of mufasa.   \n",
      "4                                          Wright voiced the iguanodon in disney 's cgi film '' dinosaur.   \n",
      "\n",
      "                                                                                            gold_atomic  \n",
      "0               Jakobshavn IsbrÃ¦ is a major contributor to the mass balance of the Greenland ice sheet.  \n",
      "1                        Jakobshavn IsbrÃ¦ releases large icebergs that pass out of the fjord each year.  \n",
      "2  Around 10% of Greenlandâ€™s icebergsâ€”about 35 billion tonnesâ€”calve and flow out of the fjord annually.  \n",
      "3              Wright played the role of Mufasa in the original Broadway production of *The Lion King*.  \n",
      "4                                    He also voiced Kron the Iguanodon in Disneyâ€™s CGI film *Dinosaur*.  \n",
      "the cat in the hat ['the', 'cat', 'in', 'the', 'hat']\n",
      "\n",
      "=== ROW-LEVEL METRICS ===\n",
      "Exact match rate          : 0.03571428571428571\n",
      "Token-level F1 (mean)     : 0.6146256448260873\n",
      "Token-level precision mean: 0.6933529460676624\n",
      "Token-level recall mean   : 0.5855549378359961\n",
      "\n",
      "=== SET-LEVEL METRICS (per sentence_id) ===\n",
      "Macro precision: 0.03743961352657004\n",
      "Macro recall   : 0.03743961352657004\n",
      "Macro F1       : 0.03743961352657004\n",
      "\n",
      "Micro precision: 0.03571428571428571\n",
      "Micro recall   : 0.03571428571428571\n",
      "Micro F1       : 0.03571428571428571\n",
      "\n",
      "=== WORST ROW-LEVEL F1 EXAMPLES ===\n",
      "     sentence_id  \\\n",
      "115          101   \n",
      "190  sentence_id   \n",
      "212          158   \n",
      "16            10   \n",
      "72            77   \n",
      "13             9   \n",
      "193          147   \n",
      "94            89   \n",
      "246          177   \n",
      "11             6   \n",
      "\n",
      "                                                                              model_atomic  \\\n",
      "115                                                                            It went up.   \n",
      "190                                                                           model_atomic   \n",
      "212                                                             He could not be explained.   \n",
      "16                          Any two unmarried people come with marriages and civil unions.   \n",
      "72                                                                Band first single feels.   \n",
      "13                                          The vicious white kids formed for one concert.   \n",
      "193                                                                     X got ta tell you.   \n",
      "94                                                              In practise she took over.   \n",
      "246  A new theoretical framework condensed during mass loss from stars of differing types.   \n",
      "11                                        The lead track combines the band 's first album.   \n",
      "\n",
      "                                                                     gold_atomic  \\\n",
      "115                         The total amount eventually increased significantly.   \n",
      "190                                                                  gold_atomic   \n",
      "212                                 Certain textual issues remained unexplained.   \n",
      "16       This arrangement grants some marriage-like rights to unmarried couples.   \n",
      "72       The bandâ€™s debut single conveys the emotional tone of their early work.   \n",
      "13                                They came together solely to perform one show.   \n",
      "193                      â€œGotta Tell Youâ€ became a major hit for Samantha Mumba.   \n",
      "94                               She ultimately assumed control of the business.   \n",
      "246  This framework described mass-loss behavior across different stellar types.   \n",
      "11       The track fuses elements of their early and more recent musical styles.   \n",
      "\n",
      "     row_token_f1  \n",
      "115      0.000000  \n",
      "190      0.000000  \n",
      "212      0.000000  \n",
      "16       0.105263  \n",
      "72       0.125000  \n",
      "13       0.125000  \n",
      "193      0.133333  \n",
      "94       0.166667  \n",
      "246      0.181818  \n",
      "11       0.190476  \n",
      "\n",
      "=== WORST SET-LEVEL F1 SENTENCE_IDS ===\n",
      "  sentence_id  n_model  n_gold  set_precision  set_recall  set_f1  \\\n",
      "0           1        3       3            0.0         0.0     0.0   \n",
      "1          10        3       3            0.0         0.0     0.0   \n",
      "2         100        2       2            0.0         0.0     0.0   \n",
      "3         101        1       1            0.0         0.0     0.0   \n",
      "4         102        2       2            0.0         0.0     0.0   \n",
      "5         103        2       2            0.0         0.0     0.0   \n",
      "6         104        2       2            0.0         0.0     0.0   \n",
      "7         105        1       1            0.0         0.0     0.0   \n",
      "8         106        2       2            0.0         0.0     0.0   \n",
      "9         107        3       3            0.0         0.0     0.0   \n",
      "\n",
      "   true_positives  \n",
      "0               0  \n",
      "1               0  \n",
      "2               0  \n",
      "3               0  \n",
      "4               0  \n",
      "5               0  \n",
      "6               0  \n",
      "7               0  \n",
      "8               0  \n",
      "9               0  \n"
     ]
    }
   ],
   "source": [
    "##evaluate with f1\n",
    "\n",
    "# Atomic Sentence Evaluation Notebook\n",
    "\n",
    "# 0. Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "# 1. Load Data --------------------------------------------------------------\n",
    "\n",
    "# ðŸ”§ CHANGE THIS to your actual file name/path\n",
    "DATA_PATH = \"../data/gold_standard.csv\"   # e.g., \"atomic_eval.csv\"\n",
    "\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "print(\"Loaded rows:\", len(df))\n",
    "print(df.head())\n",
    "\n",
    "# Sanity check for expected columns\n",
    "required_cols = {\"sentence_id\", \"model_atomic\", \"gold_atomic\"}\n",
    "missing = required_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"Missing columns in CSV: {missing}\")\n",
    "\n",
    "\n",
    "# 2. Normalization Helpers --------------------------------------------------\n",
    "\n",
    "def normalize_text(s: str) -> str:\n",
    "    \"\"\"\n",
    "    Simple normalization:\n",
    "    - lowercase\n",
    "    - strip leading/trailing spaces\n",
    "    - remove punctuation\n",
    "    - collapse multiple spaces\n",
    "    \"\"\"\n",
    "    if not isinstance(s, str):\n",
    "        return \"\"\n",
    "    s = s.lower().strip()\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n",
    "\n",
    "def tokenize(s: str):\n",
    "    return normalize_text(s).split()\n",
    "\n",
    "print(normalize_text(\"The Cat, in THE Hat!!\"), tokenize(\"The Cat, in THE Hat!!\"))\n",
    "\n",
    "\n",
    "# 3. Row-level Metrics ------------------------------------------------------\n",
    "\n",
    "def token_f1(pred: str, gold: str):\n",
    "    \"\"\"\n",
    "    Bag-of-words token overlap F1, similar to SQuAD-style.\n",
    "    \"\"\"\n",
    "    pred_toks = tokenize(pred)\n",
    "    gold_toks = tokenize(gold)\n",
    "\n",
    "    if len(pred_toks) == 0 and len(gold_toks) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    if len(pred_toks) == 0 or len(gold_toks) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    pred_counts = Counter(pred_toks)\n",
    "    gold_counts = Counter(gold_toks)\n",
    "    overlap = sum((pred_counts & gold_counts).values())\n",
    "\n",
    "    precision = overlap / max(len(pred_toks), 1)\n",
    "    recall = overlap / max(len(gold_toks), 1)\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "# Compute row-level exact match and token F1\n",
    "df[\"norm_model\"] = df[\"model_atomic\"].apply(normalize_text)\n",
    "df[\"norm_gold\"] = df[\"gold_atomic\"].apply(normalize_text)\n",
    "\n",
    "df[\"exact_match\"] = (df[\"norm_model\"] == df[\"norm_gold\"]).astype(int)\n",
    "\n",
    "prs, rcs, f1s = [], [], []\n",
    "for m, g in zip(df[\"model_atomic\"], df[\"gold_atomic\"]):\n",
    "    p, r, f = token_f1(m, g)\n",
    "    prs.append(p)\n",
    "    rcs.append(r)\n",
    "    f1s.append(f)\n",
    "\n",
    "df[\"row_token_precision\"] = prs\n",
    "df[\"row_token_recall\"] = rcs\n",
    "df[\"row_token_f1\"] = f1s\n",
    "\n",
    "print(\"\\n=== ROW-LEVEL METRICS ===\")\n",
    "print(\"Exact match rate          :\", df[\"exact_match\"].mean())\n",
    "print(\"Token-level F1 (mean)     :\", df[\"row_token_f1\"].mean())\n",
    "print(\"Token-level precision mean:\", df[\"row_token_precision\"].mean())\n",
    "print(\"Token-level recall mean   :\", df[\"row_token_recall\"].mean())\n",
    "\n",
    "df.head()\n",
    "\n",
    "\n",
    "# 4. Group-level Set Metrics (per sentence_id) ------------------------------\n",
    "\n",
    "# Build sets of normalized sentences per sentence_id\n",
    "grouped = defaultdict(lambda: {\"gold\": set(), \"model\": set()})\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    sid = row[\"sentence_id\"]\n",
    "    g = normalize_text(row[\"gold_atomic\"])\n",
    "    m = normalize_text(row[\"model_atomic\"])\n",
    "    if g:\n",
    "        grouped[sid][\"gold\"].add(g)\n",
    "    if m:\n",
    "        grouped[sid][\"model\"].add(m)\n",
    "\n",
    "def prf_from_sets(model_set, gold_set):\n",
    "    \"\"\"\n",
    "    Precision/Recall/F1 from two sets of strings (model vs gold).\n",
    "    \"\"\"\n",
    "    if len(model_set) == 0 and len(gold_set) == 0:\n",
    "        return 1.0, 1.0, 1.0\n",
    "    if len(model_set) == 0 or len(gold_set) == 0:\n",
    "        return 0.0, 0.0, 0.0\n",
    "\n",
    "    inter = model_set & gold_set\n",
    "    tp = len(inter)\n",
    "    precision = tp / len(model_set) if model_set else 0.0\n",
    "    recall = tp / len(gold_set) if gold_set else 0.0\n",
    "    if precision + recall == 0:\n",
    "        f1 = 0.0\n",
    "    else:\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "    return precision, recall, f1\n",
    "\n",
    "rows = []\n",
    "micro_tp = micro_pred = micro_gold = 0\n",
    "\n",
    "for sid, d in grouped.items():\n",
    "    mset, gset = d[\"model\"], d[\"gold\"]\n",
    "    p, r, f = prf_from_sets(mset, gset)\n",
    "\n",
    "    inter = mset & gset\n",
    "    micro_tp += len(inter)\n",
    "    micro_pred += len(mset)\n",
    "    micro_gold += len(gset)\n",
    "\n",
    "    rows.append({\n",
    "        \"sentence_id\": sid,\n",
    "        \"n_model\": len(mset),\n",
    "        \"n_gold\": len(gset),\n",
    "        \"set_precision\": p,\n",
    "        \"set_recall\": r,\n",
    "        \"set_f1\": f,\n",
    "        \"true_positives\": len(inter),\n",
    "    })\n",
    "\n",
    "set_df = pd.DataFrame(rows).sort_values(\"sentence_id\").reset_index(drop=True)\n",
    "\n",
    "macro_p = set_df[\"set_precision\"].mean()\n",
    "macro_r = set_df[\"set_recall\"].mean()\n",
    "macro_f = set_df[\"set_f1\"].mean()\n",
    "\n",
    "micro_p = micro_tp / micro_pred if micro_pred else 0.0\n",
    "micro_r = micro_tp / micro_gold if micro_gold else 0.0\n",
    "micro_f = 2 * micro_p * micro_r / (micro_p + micro_r) if (micro_p + micro_r) else 0.0\n",
    "\n",
    "print(\"\\n=== SET-LEVEL METRICS (per sentence_id) ===\")\n",
    "print(\"Macro precision:\", macro_p)\n",
    "print(\"Macro recall   :\", macro_r)\n",
    "print(\"Macro F1       :\", macro_f)\n",
    "print()\n",
    "print(\"Micro precision:\", micro_p)\n",
    "print(\"Micro recall   :\", micro_r)\n",
    "print(\"Micro F1       :\", micro_f)\n",
    "\n",
    "set_df.head()\n",
    "\n",
    "\n",
    "# 5. Inspect worst cases ----------------------------------------------------\n",
    "\n",
    "print(\"\\n=== WORST ROW-LEVEL F1 EXAMPLES ===\")\n",
    "worst_rows = df.sort_values(\"row_token_f1\").head(10)\n",
    "print(worst_rows[[\"sentence_id\", \"model_atomic\", \"gold_atomic\", \"row_token_f1\"]])\n",
    "\n",
    "print(\"\\n=== WORST SET-LEVEL F1 SENTENCE_IDS ===\")\n",
    "print(set_df.sort_values(\"set_f1\").head(10))\n",
    "\n",
    "\n",
    "# 6. Helper to inspect one sentence_id in detail ----------------------------\n",
    "\n",
    "def inspect_sentence_id(sid):\n",
    "    \"\"\"\n",
    "    Show all model/gold pairs for a given sentence_id + set-level TP/FP/FN.\n",
    "    \"\"\"\n",
    "    subset = df[df[\"sentence_id\"] == sid].copy()\n",
    "    if subset.empty:\n",
    "        print(f\"No rows with sentence_id={sid}\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\n================ sentence_id = {sid} ================\\n\")\n",
    "    print(\"Row-wise pairs:\")\n",
    "    display(subset[[\"model_atomic\", \"gold_atomic\", \"row_token_f1\"]])\n",
    "\n",
    "    mset = {normalize_text(x) for x in subset[\"model_atomic\"] if isinstance(x, str)}\n",
    "    gset = {normalize_text(x) for x in subset[\"gold_atomic\"] if isinstance(x, str)}\n",
    "    inter = mset & gset\n",
    "\n",
    "    print(\"\\nGold set (normalized):\")\n",
    "    for g in gset:\n",
    "        print(\"  G:\", g)\n",
    "\n",
    "    print(\"\\nModel set (normalized):\")\n",
    "    for m in mset:\n",
    "        print(\"  M:\", m)\n",
    "\n",
    "    print(\"\\nTrue positives (intersection):\")\n",
    "    for t in inter:\n",
    "        print(\"  TP:\", t)\n",
    "\n",
    "    print(\"\\nFalse negatives (gold but not model):\")\n",
    "    for fn in sorted(gset - inter):\n",
    "        print(\"  FN:\", fn)\n",
    "\n",
    "    print(\"\\nFalse positives (model but not gold):\")\n",
    "    for fp in sorted(mset - inter):\n",
    "        print(\"  FP:\", fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================\n",
    "# Cell 1 â€” Setup & Imports\n",
    "# ============================\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "\n",
    "# For sanity-check display\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Cell 2 â€” Load Data\n",
    "# ============================\n",
    "\n",
    "# CHANGE THIS if your file is named differently\n",
    "csv_path = \"atomic_eval_pairs.csv\"\n",
    "\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "expected_cols = {\"model_atomic\", \"gold_atomic\"}\n",
    "missing = expected_cols - set(df.columns)\n",
    "if missing:\n",
    "    raise ValueError(f\"CSV is missing columns: {missing}. Columns found: {df.columns.tolist()}\")\n",
    "\n",
    "print(f\"Loaded rows: {len(df)}\")\n",
    "print(df.head())\n",
    "\n",
    "\n",
    "# Optional: clean up whitespace (but keep case & content)\n",
    "def normalize_text(s):\n",
    "    if isinstance(s, str):\n",
    "        return \" \".join(s.split())\n",
    "    return \"\"\n",
    "\n",
    "df[\"model_atomic_norm\"] = df[\"model_atomic\"].apply(normalize_text)\n",
    "df[\"gold_atomic_norm\"]  = df[\"gold_atomic\"].apply(normalize_text)\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Cell 3 â€” ROUGE (1, 2, L)\n",
    "# ============================\n",
    "\n",
    "\"\"\"\n",
    "We compute ROUGE-1, ROUGE-2, and ROUGE-L between each pair of\n",
    "(model_atomic_norm, gold_atomic_norm), then average precision/recall/F1.\n",
    "\"\"\"\n",
    "\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "\n",
    "rouge1_p, rouge1_r, rouge1_f = [], [], []\n",
    "rouge2_p, rouge2_r, rouge2_f = [], [], []\n",
    "rougel_p, rougel_r, rougel_f = [], [], []\n",
    "\n",
    "for m, g in zip(df[\"model_atomic_norm\"], df[\"gold_atomic_norm\"]):\n",
    "    scores = scorer.score(g, m)  # (reference, prediction)\n",
    "\n",
    "    r1 = scores[\"rouge1\"]\n",
    "    r2 = scores[\"rouge2\"]\n",
    "    rl = scores[\"rougeL\"]\n",
    "\n",
    "    rouge1_p.append(r1.precision)\n",
    "    rouge1_r.append(r1.recall)\n",
    "    rouge1_f.append(r1.fmeasure)\n",
    "\n",
    "    rouge2_p.append(r2.precision)\n",
    "    rouge2_r.append(r2.recall)\n",
    "    rouge2_f.append(r2.fmeasure)\n",
    "\n",
    "    rougel_p.append(rl.precision)\n",
    "    rougel_r.append(rl.recall)\n",
    "    rougel_f.append(rl.fmeasure)\n",
    "\n",
    "# Add per-row ROUGE-L F1 to the dataframe (often the most interpretable)\n",
    "df[\"rougeL_f\"] = rougel_f\n",
    "\n",
    "print(\"\\n=== ROUGE (averaged over all pairs) ===\")\n",
    "print(f\"ROUGE-1 Precision: {np.mean(rouge1_p):.4f}\")\n",
    "print(f\"ROUGE-1 Recall   : {np.mean(rouge1_r):.4f}\")\n",
    "print(f\"ROUGE-1 F1       : {np.mean(rouge1_f):.4f}\\n\")\n",
    "\n",
    "print(f\"ROUGE-2 Precision: {np.mean(rouge2_p):.4f}\")\n",
    "print(f\"ROUGE-2 Recall   : {np.mean(rouge2_r):.4f}\")\n",
    "print(f\"ROUGE-2 F1       : {np.mean(rouge2_f):.4f}\\n\")\n",
    "\n",
    "print(f\"ROUGE-L Precision: {np.mean(rougel_p):.4f}\")\n",
    "print(f\"ROUGE-L Recall   : {np.mean(rougel_r):.4f}\")\n",
    "print(f\"ROUGE-L F1       : {np.mean(rougel_f):.4f}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Cell 4 â€” BERTScore\n",
    "# ============================\n",
    "\n",
    "\"\"\"\n",
    "BERTScore compares model vs. gold at a semantic level using a pretrained LM.\n",
    "We use English defaults and rescale_with_baseline=True (common in papers).\n",
    "\"\"\"\n",
    "\n",
    "cands = df[\"model_atomic_norm\"].tolist()\n",
    "refs  = df[\"gold_atomic_norm\"].tolist()\n",
    "\n",
    "P, R, F1 = bert_score(cands, refs, lang=\"en\", rescale_with_baseline=True)\n",
    "\n",
    "# Convert tensors to floats\n",
    "df[\"bertscore_P\"]  = P.numpy()\n",
    "df[\"bertscore_R\"]  = R.numpy()\n",
    "df[\"bertscore_F1\"] = F1.numpy()\n",
    "\n",
    "print(\"\\n=== BERTScore (averaged over all pairs) ===\")\n",
    "print(f\"BERTScore Precision: {df['bertscore_P'].mean():.4f}\")\n",
    "print(f\"BERTScore Recall   : {df['bertscore_R'].mean():.4f}\")\n",
    "print(f\"BERTScore F1       : {df['bertscore_F1'].mean():.4f}\")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Cell 5 â€” Inspect Worst / Best Examples\n",
    "# ============================\n",
    "\n",
    "# Sort by BERTScore F1 (ascending) to see worst cases\n",
    "print(\"\\n=== 10 WORST ATOMS BY BERTScore F1 ===\")\n",
    "cols_to_show = [\"sentence_id\"] if \"sentence_id\" in df.columns else []\n",
    "cols_to_show += [\"model_atomic\", \"gold_atomic\", \"bertscore_F1\", \"rougeL_f\"]\n",
    "\n",
    "display(\n",
    "    df.sort_values(\"bertscore_F1\").head(10)[cols_to_show]\n",
    ")\n",
    "\n",
    "print(\"\\n=== 10 BEST ATOMS BY BERTScore F1 ===\")\n",
    "display(\n",
    "    df.sort_values(\"bertscore_F1\", ascending=False).head(10)[cols_to_show]\n",
    ")\n",
    "\n",
    "\n",
    "# ============================\n",
    "# Cell 6 â€” Save with Metrics\n",
    "# ============================\n",
    "\n",
    "out_path = \"atomic_eval_with_rouge_bertscore.csv\"\n",
    "df.to_csv(out_path, index=False)\n",
    "print(f\"\\nSaved detailed results to: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
