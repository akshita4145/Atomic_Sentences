{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load spacy in\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Only ADJ advmod managers\n",
      "managers NOUN nsubj approve\n",
      "who PRON nsubj completed\n",
      "completed VERB relcl managers\n",
      "training NOUN dobj completed\n",
      "can AUX aux approve\n",
      "approve VERB ROOT approve\n",
      "budget NOUN compound requests\n",
      "requests NOUN dobj approve\n",
      ". PUNCT punct approve\n"
     ]
    }
   ],
   "source": [
    "##EXAMPLE PARSED SENTENCE WITH SPACY\n",
    "text = \"Only managers who completed training can approve budget requests.\"\n",
    "doc = nlp(text)\n",
    "for token in doc:\n",
    "    print(token.text, token.pos_, token.dep_, token.head.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     13\u001b[39m         processed.append({\n\u001b[32m     14\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msentence_id\u001b[39m\u001b[33m\"\u001b[39m: idx,\n\u001b[32m     15\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33msentence_text\u001b[39m\u001b[33m\"\u001b[39m: sentence,\n\u001b[32m   (...)\u001b[39m\u001b[32m     21\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis_punct\u001b[39m\u001b[33m\"\u001b[39m: token.is_punct,\n\u001b[32m     22\u001b[39m         })\n\u001b[32m     24\u001b[39m processed_df = pd.DataFrame(processed)\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m \u001b[43mprocessed\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_csv\u001b[49m(\u001b[33m'\u001b[39m\u001b[33m../data/tokenized_data.csv\u001b[39m\u001b[33m'\u001b[39m, index=\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mAttributeError\u001b[39m: 'list' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/train_subset_cleaned.csv')\n",
    "cleaned = df['cleaned_text']\n",
    "\n",
    "processed = []\n",
    "\n",
    "for idx, sentence in enumerate(cleaned):\n",
    "    doc = nlp(sentence)\n",
    "    \n",
    "    # Iterate over each token in the parsed sentence\n",
    "    for token in doc:\n",
    "        processed.append({\n",
    "            \"sentence_id\": idx,\n",
    "            \"sentence_text\": sentence,\n",
    "            \"token_text\": token.text,\n",
    "            \"pos_tag\": token.pos_,\n",
    "            \"dep_relation\": token.dep_,\n",
    "            \"lemma\": token.lemma_,\n",
    "            \"is_stop\": token.is_stop,\n",
    "            \"is_punct\": token.is_punct,\n",
    "        })\n",
    "\n",
    "processed_df = pd.DataFrame(processed)\n",
    "processed_df.to_csv('../data/tokenized_data.csv', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
