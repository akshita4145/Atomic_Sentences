{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import spacy as sp\n",
    "\n",
    "tokens = pd.read_csv(\"../data/tokenized_data.csv\")\n",
    "nlp = sp.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"read\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def clean_clause(tokens):\n",
    "    text = \" \".join(t.text for t in tokens)\n",
    "    text = text.replace(\" ,\", \",\").replace(\"  \", \" \").strip()\n",
    "    return text\n",
    "\n",
    "#NP core without appositions/relcl\n",
    "def _np_core(head):\n",
    "    keep = {head}\n",
    "    for c in head.children:\n",
    "        if c.dep_ in (\"det\",\"amod\",\"poss\",\"compound\",\"nummod\",\"flat\",\"fixed\"):\n",
    "            keep.add(c)\n",
    "        #allow 'of' PP inside NP (e.g., \"place of birth\")\n",
    "        if c.dep_ == \"prep\" and c.lemma_ == \"of\":\n",
    "            keep.update(list(c.subtree))\n",
    "    toks = sorted(keep, key=lambda x: x.i)\n",
    "    return toks\n",
    "\n",
    "def _subject_token(root):\n",
    "    subs = [t for t in root.lefts if t.dep_ in (\"nsubj\",\"nsubjpass\")]\n",
    "    return subs[0] if subs else None\n",
    "\n",
    "def _subject_text(subj):\n",
    "    if subj is None: return \"X\"\n",
    "    keep = {subj}\n",
    "    for c in subj.children:\n",
    "        if c.dep_ in (\"det\",\"amod\",\"poss\",\"compound\",\"nummod\",\"flat\",\"fixed\"):\n",
    "            keep.add(c)\n",
    "    toks = sorted(keep, key=lambda x: x.i)\n",
    "    return clean_clause(toks)\n",
    "\n",
    "#core verb phrase\n",
    "def _verb_phrase_core(root):\n",
    "    keep = {root}\n",
    "    for c in root.children:\n",
    "        if c.dep_ in (\"aux\",\"auxpass\",\"neg\",\"prt\",\"mark\"):\n",
    "            keep.add(c)\n",
    "        if c.dep_ in (\"dobj\",\"attr\"):\n",
    "            keep.update(list(c.subtree))\n",
    "        #only include 'to' + NP core of its pobj (no appos/relcl)\n",
    "        if c.dep_ == \"prep\" and c.lemma_ == \"to\":\n",
    "            pobj = next((gc for gc in c.children if gc.dep_==\"pobj\"), None)\n",
    "            if pobj is not None:\n",
    "                keep.add(c)\n",
    "                keep.update(_np_core(pobj))\n",
    "        #xcomp chain (keep its objs and 'to'-PP NP core)\n",
    "        if c.dep_ == \"xcomp\" and c.pos_ == \"VERB\":\n",
    "            keep.add(c)\n",
    "            for rc in c.children:\n",
    "                if rc.dep_ in (\"aux\",\"auxpass\",\"neg\",\"prt\",\"mark\"):\n",
    "                    keep.add(rc)\n",
    "                if rc.dep_ in (\"dobj\",\"attr\"):\n",
    "                    keep.update(list(rc.subtree))\n",
    "                if rc.dep_ == \"prep\" and rc.lemma_ == \"to\":\n",
    "                    pobj2 = next((gc for gc in rc.children if gc.dep_==\"pobj\"), None)\n",
    "                    if pobj2 is not None:\n",
    "                        keep.add(rc)\n",
    "                        keep.update(_np_core(pobj2))\n",
    "    toks = sorted(keep, key=lambda x: x.i)\n",
    "    return toks  #return tokens so we can clip later\n",
    "\n",
    "def _capitalize(s):\n",
    "    return s[0].upper() + s[1:] if s else s\n",
    "\n",
    "def _endsent(s):\n",
    "    s = s.rstrip()\n",
    "    return s if s.endswith(('.', '!', '?')) else s + \".\"\n",
    "\n",
    "#build a clause from a (finite or participial) head and borrow subject if needed\n",
    "def _finite_clause_from(head, borrow_subj_txt):\n",
    "    keep = {head}\n",
    "    for c in head.children:\n",
    "        if c.dep_ in (\"aux\",\"auxpass\",\"neg\",\"prt\",\"mark\"):\n",
    "            keep.add(c)\n",
    "        if c.dep_ in (\"nsubj\",\"nsubjpass\",\"dobj\",\"attr\",\"ccomp\",\"xcomp\"):\n",
    "            keep.update(list(c.subtree))\n",
    "        if c.dep_ == \"prep\":\n",
    "            keep.update(list(c.subtree))\n",
    "        #include coordinated verb and its arguments\n",
    "        if c.dep_ == \"conj\" and c.pos_ == \"VERB\":\n",
    "            keep.add(c)\n",
    "            for gc in c.children:\n",
    "                if gc.dep_ in (\"aux\",\"auxpass\",\"neg\",\"prt\",\"nsubj\",\"nsubjpass\",\"dobj\",\"attr\",\"ccomp\",\"xcomp\",\"prep\"):\n",
    "                    keep.update(list(gc.subtree))\n",
    "\n",
    "    toks = sorted(keep, key=lambda x: x.i)\n",
    "\n",
    "    has_subj = any(t.dep_ in (\"nsubj\",\"nsubjpass\") for t in toks)\n",
    "    sent_text = \" \".join(t.text for t in toks)\n",
    "    if not has_subj and borrow_subj_txt:\n",
    "        sent_text = f\"{borrow_subj_txt} \" + sent_text\n",
    "\n",
    "    #drop sentence-initial relativizers/markers\n",
    "    for w in (\"that\",\"which\",\"who\",\"whom\",\"where\",\"when\"):\n",
    "        if sent_text.lower().startswith(w + \" \"):\n",
    "            sent_text = sent_text[len(w)+1:]\n",
    "            break\n",
    "\n",
    "    sent_text = sent_text.replace(\" ,\", \",\").replace(\"  \", \" \").strip()\n",
    "    if sent_text and not sent_text.endswith((\".\", \"!\", \"?\")):\n",
    "        sent_text += \".\"\n",
    "    if sent_text:\n",
    "        sent_text = sent_text[0].upper() + sent_text[1:]\n",
    "    return sent_text\n",
    "\n",
    "def atomic_sentence_extract(doc):\n",
    "    atomic_sentences = []\n",
    "\n",
    "    for sentence in doc.sents:\n",
    "        root = sentence.root\n",
    "\n",
    "        #main\n",
    "        subj_tok = _subject_token(root)\n",
    "        subj_txt = _subject_text(subj_tok)\n",
    "\n",
    "        vp_toks = _verb_phrase_core(root)\n",
    "\n",
    "        #drop any material before the subject in the main clause\n",
    "        if subj_tok is not None:\n",
    "            vp_toks = [t for t in vp_toks if t.i >= subj_tok.i or t == root or t.head == root]\n",
    "\n",
    "        vp_txt = clean_clause(vp_toks)\n",
    "\n",
    "        #trim leading temporal openers if still present\n",
    "        if vp_txt.lower().startswith((\"after \", \"before \", \"because \", \"while \", \"when \")):\n",
    "            pos = vp_txt.lower().find(subj_txt.lower())\n",
    "            if pos > 0:\n",
    "                vp_txt = vp_txt[pos:]\n",
    "            elif \",\" in vp_txt:\n",
    "                vp_txt = vp_txt.split(\",\", 1)[1].strip()\n",
    "\n",
    "        main_text = vp_txt if vp_txt.lower().startswith(subj_txt.lower()) else f\"{subj_txt} {vp_txt}\"\n",
    "        main_text = _endsent(_capitalize(main_text))\n",
    "        if len(main_text.split()) >= 3:\n",
    "            atomic_sentences.append(main_text)\n",
    "\n",
    "        #appositive fact (to)\n",
    "        roots_pobj = None\n",
    "        for c in root.children:\n",
    "            if c.dep_ == \"prep\" and c.lemma_ == \"to\":\n",
    "                roots_pobj = next((gc for gc in c.children if gc.dep_==\"pobj\"), None)\n",
    "                if roots_pobj: break\n",
    "\n",
    "        if roots_pobj is not None:\n",
    "            appos_list = [ch for ch in roots_pobj.children if ch.dep_==\"appos\" and ch.pos_ in (\"NOUN\",\"PROPN\")]\n",
    "            if appos_list:\n",
    "                anchor_txt = clean_clause(_np_core(roots_pobj))\n",
    "                right = [clean_clause(_np_core(a)) for a in appos_list]\n",
    "                cop = _endsent(_capitalize(f\"{anchor_txt} are \" + \", \".join(right)))\n",
    "                if len(cop.split()) >= 3:\n",
    "                    atomic_sentences.append(cop)\n",
    "\n",
    "        #looks at relative clauses (splits into types of relcl later on)\n",
    "        for rel in [t for t in sentence if t.dep_==\"relcl\"]:\n",
    "            rkeep = {rel}\n",
    "            for rc in rel.children:\n",
    "                if rc.dep_ in (\"aux\",\"auxpass\",\"neg\",\"prt\",\"dobj\",\"attr\",\"ccomp\",\"xcomp\"):\n",
    "                    rkeep.update(list(rc.subtree))\n",
    "                if rc.dep_ == \"prep\":\n",
    "                    rkeep.update(list(rc.subtree))\n",
    "            r_toks = sorted(rkeep, key=lambda x: x.i)\n",
    "            #strip relativizers\n",
    "            rel_words = [t for t in r_toks if t.text.lower() not in {\"where\",\"that\",\"who\",\"which\"} and t.dep_ != \"mark\"]\n",
    "            if not any(t.pos_ == \"VERB\" for t in rel_words):\n",
    "                continue\n",
    "            rel_text = clean_clause(rel_words)\n",
    "            #ensure subject if missing\n",
    "            if not any(t.dep_ in (\"nsubj\",\"nsubjpass\") for t in rel_words):\n",
    "                rel_text = f\"{('He' if subj_tok is None or subj_tok.pos_=='PROPN' else subj_txt)} {rel_text}\"\n",
    "            rel_text = _endsent(_capitalize(rel_text))\n",
    "            if len(rel_text.split()) >= 3:\n",
    "                atomic_sentences.append(rel_text)\n",
    "\n",
    "        #participial/adverbial clause\n",
    "        for tok in sentence:\n",
    "            is_participle = (tok.pos_ == \"VERB\" and tok.tag_ == \"VBG\")\n",
    "            looks_clausey = tok.dep_ in (\"advcl\",\"acl\",\"conj\") and tok.head == root\n",
    "            if is_participle or looks_clausey:\n",
    "                if tok.lemma_ in {\"produce\",\"contribute\",\"generate\",\"account\"} or is_participle:\n",
    "                    produced = _finite_clause_from(tok, borrow_subj_txt=subj_txt)\n",
    "                    # require a number or an object-ish cue to avoid junk\n",
    "                    if any(ch.isdigit() for ch in produced) or any(w in produced.lower() for w in (\"percent\",\"%\",\"tonne\",\"billion\",\"million\",\"all\",\"of \")):\n",
    "                        atomic_sentences.append(produced)\n",
    "\n",
    "        #coordinated finite clauses\n",
    "        for tok in sentence:\n",
    "            is_finite_like = (tok.pos_ == \"VERB\" and tok.morph.get(\"VerbForm\") != [\"Inf\"])\n",
    "            if is_finite_like and tok != root and tok.dep_ in (\"conj\",\"parataxis\"):\n",
    "                extra = _finite_clause_from(tok, borrow_subj_txt=subj_txt)\n",
    "                if extra and len(extra.split()) >= 4:\n",
    "                    atomic_sentences.append(extra)\n",
    "\n",
    "    #strong deduplication\n",
    "    seen, final = set(), []\n",
    "    for s in atomic_sentences:\n",
    "        key = \" \".join(s.lower().split())\n",
    "        if key not in seen:\n",
    "            seen.add(key); final.append(s)\n",
    "    return final\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for idx, row in tokens.iterrows(): \n",
    "    sentence_id = row[\"sentence_id\"] \n",
    "    sentence_text = row[\"sentence_text\"] \n",
    "    doc = nlp(sentence_text) \n",
    "    atomic_sentences = atomic_sentence_extract(doc) \n",
    "    \n",
    "    for s in atomic_sentences: \n",
    "        all_sentences.append({ \"sentence_id\": sentence_id, \"atomic_sentence\": s }) \n",
    "        \n",
    "import pandas as pd \n",
    "out_df = pd.DataFrame(all_sentences).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atomic sentences extraction complete. Saved to 'split_sentences_v2.csv'.\n"
     ]
    }
   ],
   "source": [
    "df_atomic = pd.DataFrame(all_sentences)\n",
    "\n",
    "df_atomic.to_csv('../data/split_sentences_v2.csv', index=False)\n",
    "\n",
    "print(\"Atomic sentences extraction complete. Saved to 'split_sentences_v2.csv'.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
